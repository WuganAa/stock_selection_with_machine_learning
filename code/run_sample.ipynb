{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"F:\\\\DeepLearning\\\\Data\"\n",
    "fpath_insample = os.path.join(fpath, \"insample\")\n",
    "fpath_outsample = os.path.join(fpath, \"outsample\")\n",
    "\n",
    "X_train = np.load(os.path.join(fpath_insample, \"X.npy\"))\n",
    "Y_train = np.load(os.path.join(fpath_insample, \"Y.npy\"))\n",
    "X_test = np.load(os.path.join(fpath_outsample, \"X.npy\"))\n",
    "Y_test = np.load(os.path.join(fpath_outsample, \"Y.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70440, 229)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70440,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25500, 229)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25500,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_sample = \"F:\\\\DeepLearning\\\\Data\\\\sample\"\n",
    "\n",
    "X = np.load(os.path.join(fpath_sample, \"X.npy\"))\n",
    "Y = np.load(os.path.join(fpath_sample, \"Y.npy\"))\n",
    "X_train = X[:800,:]\n",
    "Y_train = Y[:800]\n",
    "X_test = X[800:,:]\n",
    "Y_test = Y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 229)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370, 229)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(370,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogisticRegression\n",
    "* RandomForest\n",
    "* SupportVectorMachine\n",
    "* DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import model \n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we begin with empty model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(X_train, Y_train, X_test, Y_test)\n",
    "rf = RandomForest(X_train, Y_train, X_test, Y_test)\n",
    "svm = SupportVectorMachine(X_train, Y_train, X_test, Y_test)\n",
    "dnn = DNN(X_train, Y_train, X_test, Y_test)\n",
    "modelsList = [lg, rf, svm, dnn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if we begin with existent model in \"modelpath\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"F:\\\\DeepLearning\\\\Model\\\\20180929-141510\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(fpath=modelpath)\n",
    "rf = RandomForest(fpath=modelpath)\n",
    "svm = SupportVectorMachine(fpath=modelpath)\n",
    "dnn = DNN(fpath=modelpath)\n",
    "modelsList = [lg, rf, svm, dnn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = os.path.join(\"F:\\\\DeepLearning\\\\Model\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def using_model(model):\n",
    "    print(\"Begin Model: {}\\n\\n\".format(model.type))\n",
    "    Accuracy, Precision, Recall, F1, TPR, FPR, AUC = model.evalution()\n",
    "    print(\"\\nAccuracy, Precision, Recall, F1, TPR, FPR, AUC:\")\n",
    "    print(Accuracy, Precision, Recall, F1, TPR, FPR, AUC,\"\\n\")\n",
    "    \n",
    "    Accuracy, Precision, Recall, F1, TPR, FPR, AUC = model.evalution_with_data(X_test, Y_test)\n",
    "    print(\"\\nAccuracy, Precision, Recall, F1, TPR, FPR, AUC:\")\n",
    "    print(Accuracy, Precision, Recall, F1, TPR, FPR, AUC,\"\\n\")\n",
    "    \n",
    "    print(\"\\nPredict X_test with classified output:\")\n",
    "    print(model.predict(X_test[0:10,:]),\"\\n\")\n",
    "    \n",
    "    print(\"\\nPredict X_test with probability output:\")\n",
    "    print(model.predict_proba(X_test[0:10,:]),\"\\n\")\n",
    "    \n",
    "    model.save_model(modelpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Model: ModelType.LR\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 0s 362us/step - loss: 0.8226 - acc: 0.7363\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.7119 - acc: 0.8737\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.6372 - acc: 0.9150\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5850 - acc: 0.9387\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5524 - acc: 0.9400\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5302 - acc: 0.9350\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 51us/step - loss: 0.5129 - acc: 0.9612\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.5016 - acc: 0.9475\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.4922 - acc: 0.9500\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.4853 - acc: 0.9575\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4812 - acc: 0.9562\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.4771 - acc: 0.9537\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4739 - acc: 0.9575\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4718 - acc: 0.9512\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4703 - acc: 0.9587\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.4694 - acc: 0.9587\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4676 - acc: 0.9525\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.4655 - acc: 0.9612\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4650 - acc: 0.9600\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4647 - acc: 0.9550\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4627 - acc: 0.9587\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 56us/step - loss: 0.4627 - acc: 0.9587\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4618 - acc: 0.9537\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4610 - acc: 0.9500\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4602 - acc: 0.9537\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4598 - acc: 0.9650\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4600 - acc: 0.9562\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.4592 - acc: 0.9587\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4587 - acc: 0.9612\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 50us/step - loss: 0.4584 - acc: 0.9587\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 55us/step - loss: 0.4577 - acc: 0.9612\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4581 - acc: 0.9537\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 54us/step - loss: 0.4578 - acc: 0.9612\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4572 - acc: 0.9562\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4565 - acc: 0.9587\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4568 - acc: 0.9587\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.4561 - acc: 0.9612\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4565 - acc: 0.9612\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4558 - acc: 0.9600\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4556 - acc: 0.9600\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4554 - acc: 0.9650\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.4561 - acc: 0.9525\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4554 - acc: 0.9587\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4552 - acc: 0.9537\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4553 - acc: 0.9625\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 56us/step - loss: 0.4551 - acc: 0.9575\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4551 - acc: 0.9612\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4544 - acc: 0.9562\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4545 - acc: 0.9650\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 52us/step - loss: 0.4547 - acc: 0.9575\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4543 - acc: 0.9600\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4544 - acc: 0.9550\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4534 - acc: 0.9575\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4541 - acc: 0.9575\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4533 - acc: 0.9612\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4529 - acc: 0.9550\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4538 - acc: 0.9587\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4537 - acc: 0.9575\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4532 - acc: 0.9575\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 46us/step - loss: 0.4537 - acc: 0.9612\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4533 - acc: 0.9637\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4531 - acc: 0.9537\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4526 - acc: 0.9600\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4526 - acc: 0.9562\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4526 - acc: 0.9562\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4529 - acc: 0.9550\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4530 - acc: 0.9587\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4527 - acc: 0.9575\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4533 - acc: 0.9625\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.4523 - acc: 0.9575\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4528 - acc: 0.9587\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4523 - acc: 0.9575\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4525 - acc: 0.9550\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4524 - acc: 0.9612\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4524 - acc: 0.9587\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4521 - acc: 0.9537\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4519 - acc: 0.9587\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4521 - acc: 0.9550\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4514 - acc: 0.9637\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4518 - acc: 0.9575\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4517 - acc: 0.9575\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4523 - acc: 0.9587\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4513 - acc: 0.9587\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 47us/step - loss: 0.4512 - acc: 0.9600\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 49us/step - loss: 0.4517 - acc: 0.9625\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4524 - acc: 0.9637\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4511 - acc: 0.9625\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4518 - acc: 0.9587\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4510 - acc: 0.9487\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4516 - acc: 0.9650\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 45us/step - loss: 0.4516 - acc: 0.9537\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 40us/step - loss: 0.4516 - acc: 0.9612\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4511 - acc: 0.9562\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4509 - acc: 0.9612\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4507 - acc: 0.9512\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4511 - acc: 0.9575\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.4500 - acc: 0.9612\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.4503 - acc: 0.9575\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4514 - acc: 0.9587\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.4510 - acc: 0.9525\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9378378378378378 0.9057591623036649 0.9719101123595506 0.9376693766937669 0.9719101123595506 0.09375 0.9730220037453183 \n",
      "\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9378378378378378 0.9057591623036649 0.9719101123595506 0.9376693766937669 0.9719101123595506 0.09375 0.9730220037453183 \n",
      "\n",
      "\n",
      "Predict X_test with classified output:\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Predict X_test with probability output:\n",
      "[0.7091513  0.77372116 0.07582279 0.03318183 0.36970213 0.62172383\n",
      " 0.8352809  0.23324905 0.33412358 0.4063767 ] \n",
      "\n",
      "The LogisticRegression Model save in \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\LR_model_weights.h5 and \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\LR_model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "using_model(modelsList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Model: ModelType.RF\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9945945945945946 0.9888888888888889 1.0 0.9944134078212291 1.0 0.010416666666666666 0.9997073970037453 \n",
      "\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9945945945945946 0.9888888888888889 1.0 0.9944134078212291 1.0 0.010416666666666666 0.9947916666666667 \n",
      "\n",
      "\n",
      "Predict X_test with classified output:\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Predict X_test with probability output:\n",
      "[0.99200149 0.99923482 0.         0.         0.03997481 0.9999491\n",
      " 0.9999491  0.         0.         0.        ] \n",
      "\n",
      "The RandomForest Model save in \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\RF_model.pkl\n"
     ]
    }
   ],
   "source": [
    "using_model(modelsList[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SupportVectorMachine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Model: ModelType.SVM\n",
      "\n",
      "\n",
      "[LibSVM]\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9297297297297298 0.9 0.9606741573033708 0.9293478260869567 0.9606741573033708 0.09895833333333333 0.9801615168539326 \n",
      "\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9216216216216216 0.8669950738916257 0.9887640449438202 0.9238845144356955 0.9887640449438202 0.140625 0.9240695224719101 \n",
      "\n",
      "\n",
      "Predict X_test with classified output:\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Predict X_test with probability output:\n",
      "[9.99986377e-01 9.99996505e-01 1.97228445e-05 1.00000010e-07\n",
      " 1.81696418e-03 9.68966171e-01 9.95709092e-01 6.65320324e-03\n",
      " 3.40073709e-03 7.50821971e-02] \n",
      "\n",
      "The SupportVectorMachine Model save in \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\SVM_model.pkl\n"
     ]
    }
   ],
   "source": [
    "using_model(modelsList[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Model: ModelType.DNN\n",
      "\n",
      "\n",
      "Epoch 1/100\n",
      "800/800 [==============================] - 1s 638us/step - loss: 2.9472 - acc: 0.6513\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 0s 86us/step - loss: 2.6443 - acc: 0.8238\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 0s 92us/step - loss: 2.5959 - acc: 0.8400\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 2.4826 - acc: 0.8638\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 2.4139 - acc: 0.8850\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 2.3813 - acc: 0.8975\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 2.3174 - acc: 0.8975\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 0s 97us/step - loss: 2.2364 - acc: 0.9287\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 0s 92us/step - loss: 2.2285 - acc: 0.9175\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 0s 94us/step - loss: 2.1782 - acc: 0.9262\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 0s 91us/step - loss: 2.1335 - acc: 0.9300\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 2.1092 - acc: 0.9237\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 0s 92us/step - loss: 2.0618 - acc: 0.9412\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 0s 90us/step - loss: 2.0220 - acc: 0.9412\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 1.9713 - acc: 0.9550\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 0s 98us/step - loss: 1.9269 - acc: 0.9612\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.9038 - acc: 0.9562\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 0s 108us/step - loss: 1.8751 - acc: 0.9525\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 0s 140us/step - loss: 1.8413 - acc: 0.9550\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 0s 95us/step - loss: 1.8057 - acc: 0.9687\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 0s 105us/step - loss: 1.7788 - acc: 0.9662\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 1.7384 - acc: 0.9687\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 0s 97us/step - loss: 1.6970 - acc: 0.9825\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 0s 98us/step - loss: 1.6803 - acc: 0.9750\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 0s 79us/step - loss: 1.6508 - acc: 0.9712\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 1.6339 - acc: 0.9625\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 0s 86us/step - loss: 1.6140 - acc: 0.9725\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 1.5789 - acc: 0.9787\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 1.5406 - acc: 0.9800\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 1.5294 - acc: 0.9750\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 0s 91us/step - loss: 1.5144 - acc: 0.9712\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 0s 86us/step - loss: 1.4936 - acc: 0.9737\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 1.4555 - acc: 0.9750\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 0s 96us/step - loss: 1.4224 - acc: 0.9850\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 1.4321 - acc: 0.9712\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 0s 90us/step - loss: 1.3920 - acc: 0.9800\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 1.3552 - acc: 0.9887\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 1.3283 - acc: 0.9875\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 1.3133 - acc: 0.9862\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 1.2840 - acc: 0.9900\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 1.2908 - acc: 0.9750\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.2638 - acc: 0.9837\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 1.2449 - acc: 0.9787\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 1.2287 - acc: 0.9787\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 1.2247 - acc: 0.9787\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.1951 - acc: 0.9775\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 1.1640 - acc: 0.9862\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.1475 - acc: 0.9850\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 1.1210 - acc: 0.9875\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 1.1052 - acc: 0.9912\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 1.0898 - acc: 0.9887\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.0674 - acc: 0.9975\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 1.0702 - acc: 0.9800\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 0s 87us/step - loss: 1.0414 - acc: 0.9887\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 0s 101us/step - loss: 1.0244 - acc: 0.9900\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 0s 100us/step - loss: 1.0208 - acc: 0.9875\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - ETA: 0s - loss: 0.9959 - acc: 0.989 - 0s 100us/step - loss: 1.0004 - acc: 0.9850\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 0s 93us/step - loss: 0.9730 - acc: 0.9937\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.9928 - acc: 0.9850\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.9631 - acc: 0.9850\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.9387 - acc: 0.9875\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.9328 - acc: 0.9875\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 0s 91us/step - loss: 0.9191 - acc: 0.9862\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 0s 89us/step - loss: 0.8871 - acc: 0.9950\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.8819 - acc: 0.9887\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.8671 - acc: 0.9887\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.8579 - acc: 0.9900\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.8531 - acc: 0.9862\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.8185 - acc: 0.9950\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.8075 - acc: 0.9950\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 0s 85us/step - loss: 0.8002 - acc: 0.9912\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.7855 - acc: 0.9925\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 0s 75us/step - loss: 0.7778 - acc: 0.9950\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.7644 - acc: 0.9925\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.7674 - acc: 0.9862\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 0s 108us/step - loss: 0.7372 - acc: 0.9962\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 0s 103us/step - loss: 0.7385 - acc: 0.9900\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 0s 101us/step - loss: 0.7132 - acc: 0.9950\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 0s 96us/step - loss: 0.7246 - acc: 0.9900\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.7130 - acc: 0.9875\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 0s 94us/step - loss: 0.7214 - acc: 0.9762\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 0s 92us/step - loss: 0.6909 - acc: 0.9875\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 0s 105us/step - loss: 0.6862 - acc: 0.9812\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.6782 - acc: 0.9862\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.6680 - acc: 0.9862\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 0s 76us/step - loss: 0.6515 - acc: 0.9887\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.6280 - acc: 0.9937\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 0s 79us/step - loss: 0.6255 - acc: 0.9912\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.6219 - acc: 0.9887\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 0s 84us/step - loss: 0.6070 - acc: 0.9925\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.6013 - acc: 0.9912\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.5950 - acc: 0.9875\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 0s 79us/step - loss: 0.6099 - acc: 0.9800\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.5719 - acc: 0.9937\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 0s 86us/step - loss: 0.5630 - acc: 0.9950\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 0s 82us/step - loss: 0.5597 - acc: 0.9912\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 0s 76us/step - loss: 0.5499 - acc: 0.9912\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.5450 - acc: 0.9912\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 0s 81us/step - loss: 0.5408 - acc: 0.9925\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 0s 80us/step - loss: 0.5350 - acc: 0.9887\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9864864864864865 0.9779005524861878 0.9943820224719101 0.9860724233983287 0.9943820224719101 0.020833333333333332 0.993065308988764 \n",
      "\n",
      "\n",
      "Accuracy, Precision, Recall, F1, TPR, FPR, AUC:\n",
      "0.9864864864864865 0.9779005524861878 0.9943820224719101 0.9860724233983287 0.9943820224719101 0.020833333333333332 0.993065308988764 \n",
      "\n",
      "\n",
      "Predict X_test with classified output:\n",
      "[1. 1. 0. 0. 0. 1. 1. 0. 0. 0.] \n",
      "\n",
      "\n",
      "Predict X_test with probability output:\n",
      "[9.9965584e-01 9.9947041e-01 1.2308987e-03 2.9316230e-04 7.1185296e-03\n",
      " 9.9913990e-01 9.9875224e-01 1.3205487e-03 2.3381228e-03 2.0709084e-03] \n",
      "\n",
      "The DNN Model save in \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\DNN_model_weights.h5 and \n",
      "  F:\\DeepLearning\\Model\\20180929-145549\\DNN_model_architecture.json\n"
     ]
    }
   ],
   "source": [
    "using_model(modelsList[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"F:\\\\DeepLearning\\\\data\\\\outsample_total\"\n",
    "frequency = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_portfolio = np.load(os.path.join(fpath, \"factors_\" + str(frequency) + \"days.npy\"))\n",
    "Y_portfolio = np.load(os.path.join(fpath, \"prices_\" + str(frequency) + \"days.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelsList[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelType.DNN: 3>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolios = []\n",
    "portfolio_probas = []\n",
    "for i in range(len(X_portfolio)):\n",
    "    factors = X_portfolio[i]\n",
    "    portfolio = model.predict(factors)\n",
    "    portfolio_proba = model.predict_proba(factors)\n",
    "    portfolios.append(portfolio)\n",
    "    portfolio_probas.append(portfolio_proba)\n",
    "portfolios = np.array(portfolios)\n",
    "portfolio_probas = np.array(portfolio_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolios.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolios[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_probas[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01367685, 0.01146755, 0.6491244 , 0.08118903, 0.0157655 ,\n",
       "       0.02810515, 0.0205674 , 0.01808242, 0.01703241, 0.98353463,\n",
       "       0.94881743, 0.01701932, 0.08567779, 0.05326505, 0.06053116,\n",
       "       0.59298104, 0.01700561, 0.9657039 , 0.01692834, 0.02973276,\n",
       "       0.01717285, 0.74395263, 0.01351875, 0.01703715, 0.01561288,\n",
       "       0.2944293 , 0.03630266, 0.01512188, 0.04677643, 0.9606394 ,\n",
       "       0.86354727, 0.00783255, 0.01733734, 0.9340281 , 0.992145  ,\n",
       "       0.63586277, 0.78100336, 0.12922242, 0.06026883, 0.04903495,\n",
       "       0.89082426, 0.16718353, 0.68514013, 0.06337925, 0.9112654 ,\n",
       "       0.9208965 , 0.19730078, 0.37242836, 0.52472013, 0.01668398,\n",
       "       0.01969806, 0.2515435 , 0.96147114, 0.90284264, 0.01705395,\n",
       "       0.96003485, 0.84241974, 0.90116113, 0.95622915, 0.01837795,\n",
       "       0.01760018, 0.79733366, 0.5424491 , 0.02256664, 0.01631686,\n",
       "       0.01857221, 0.01499528, 0.01710765, 0.04741203, 0.01701227,\n",
       "       0.01704653, 0.9664052 , 0.9195778 , 0.04502695, 0.0158408 ,\n",
       "       0.69554424, 0.05636295, 0.01675494, 0.01748149, 0.01745522,\n",
       "       0.1496262 , 0.01713906, 0.01694507, 0.01761516, 0.22821763,\n",
       "       0.01742377, 0.75625134, 0.647545  , 0.9766906 , 0.18870895,\n",
       "       0.01865484, 0.01725875, 0.9778285 , 0.68030447, 0.83620965,\n",
       "       0.07586633, 0.05140512, 0.89184797, 0.797343  , 0.04202009,\n",
       "       0.82710385, 0.620296  , 0.93171126, 0.9119105 , 0.37852338,\n",
       "       0.26156175, 0.03309996, 0.01994088, 0.01646741, 0.01848271,\n",
       "       0.02770942, 0.01878891, 0.01747207, 0.01722435, 0.01675145,\n",
       "       0.01751809, 0.4742064 , 0.84094906, 0.923665  , 0.990006  ,\n",
       "       0.6634337 , 0.97026527, 0.6421282 , 0.97040075, 0.95199037,\n",
       "       0.9162203 , 0.88009655, 0.99713635, 0.9856371 , 0.17207341,\n",
       "       0.01650186, 0.75030583, 0.87514895, 0.02510363, 0.9886359 ,\n",
       "       0.92441654, 0.7964912 , 0.01610849, 0.6767368 , 0.21253015,\n",
       "       0.02135219, 0.01694572, 0.01766117, 0.12255244, 0.91187537,\n",
       "       0.04500859, 0.0170304 , 0.9715916 , 0.01641367, 0.48274675,\n",
       "       0.67522895, 0.04924645, 0.09172329, 0.92611325, 0.8233464 ,\n",
       "       0.01773593, 0.86834234, 0.01763693, 0.4532293 , 0.98312587,\n",
       "       0.01746506, 0.01736103, 0.6552571 , 0.01721083, 0.01578174,\n",
       "       0.9282881 , 0.01532197, 0.01942238, 0.18514018, 0.8963933 ,\n",
       "       0.01628152, 0.74133176, 0.02086561, 0.8329132 , 0.43893665,\n",
       "       0.54251623, 0.7424075 , 0.06408054, 0.5626998 , 0.23980802,\n",
       "       0.01696933, 0.01731301, 0.26114127, 0.14514843, 0.6999029 ,\n",
       "       0.88286936, 0.89288497, 0.9572305 , 0.01718454, 0.01097062,\n",
       "       0.01665181, 0.04615383, 0.01750124, 0.01340001, 0.4826278 ,\n",
       "       0.03541337, 0.5033191 , 0.01535209, 0.8099622 , 0.5635204 ,\n",
       "       0.11900303, 0.0412666 , 0.79752904, 0.27814445, 0.6649276 ,\n",
       "       0.00902327, 0.0147094 , 0.01688596, 0.76908255, 0.02775008,\n",
       "       0.9601132 , 0.01696294, 0.6931074 , 0.03302602, 0.01721123,\n",
       "       0.01534739, 0.0163222 , 0.11621916, 0.01341194, 0.0445136 ,\n",
       "       0.05093863, 0.24658215, 0.01610077, 0.4899078 , 0.52093107,\n",
       "       0.01719079, 0.59375674, 0.52908576, 0.01726173, 0.01646323,\n",
       "       0.01405275, 0.79333913, 0.8950167 , 0.9315398 , 0.53745174,\n",
       "       0.9186203 , 0.92148626, 0.9609612 , 0.8607592 , 0.921283  ,\n",
       "       0.97234803, 0.92276406, 0.9390664 , 0.9137485 , 0.791508  ,\n",
       "       0.9732482 , 0.93486893, 0.92136246, 0.92841446, 0.90754205,\n",
       "       0.93869823, 0.87127846, 0.952649  , 0.9953655 , 0.789757  ,\n",
       "       0.9574694 , 0.94289225, 0.88815874, 0.7470242 , 0.9737204 ,\n",
       "       0.99621767, 0.588654  , 0.9734455 , 0.91327757, 0.7746971 ,\n",
       "       0.96269727, 0.63732743, 0.26840454, 0.9917389 , 0.8632224 ,\n",
       "       0.6074802 , 0.65466106, 0.87706476, 0.9773234 , 0.86838645,\n",
       "       0.9167439 , 0.9855055 , 0.9964909 , 0.97586685, 0.6018252 ,\n",
       "       0.96401846, 0.9825378 , 0.80401486, 0.95019454, 0.96908575,\n",
       "       0.89052874, 0.9812067 , 0.98479766, 0.99600214, 0.95960724,\n",
       "       0.86648977, 0.97434413, 0.5636324 , 0.04091762, 0.5116091 ,\n",
       "       0.96955866], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portfolio_probas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"predict_DNN_5days.npy\", portfolios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"predict_proba_DNN_5days.npy\", portfolio_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
